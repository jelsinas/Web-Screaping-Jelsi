{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjY5HPO47VmALSoeWRwfXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jelsinas/Web-Screaping-Jelsi/blob/main/Jelsi_Tugas_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EKSEKUSI UTAMA - DENGAN TWEET HARVEST\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ CNBC SCRAPER + TWEET HARVEST\")\n",
        "    print(\"üéØ Target: Logam Mulia (Berita + Sosial Media)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Run testing CNBC dulu\n",
        "    if test_cnbc_structure():\n",
        "        print(\"\\nüöÄ Testing CNBC OK! Mulai scraping berita...\")\n",
        "        news_filename, news_count = scrape_cnbc_logam_mulia()\n",
        "        print(f\"\\n‚úÖ Berita selesai: {news_count} artikel di {news_filename}\")\n",
        "\n",
        "        # Opsional: Jalankan tweet harvest setelah berita\n",
        "        print(\"\\nüê¶ Mulai tweet harvest? (Ya/Tidak)\")\n",
        "        # Untuk auto: Uncomment baris di bawah\n",
        "        # tweet_filename, tweet_count = harvest_tweets_logam_mulia(max_tweets=200)\n",
        "        # print(f\"‚úÖ Tweet selesai: {tweet_count} tweet di {tweet_filename}\")\n",
        "\n",
        "        # Atau manual: Jalankan fungsi terpisah\n",
        "    else:\n",
        "        print(\"\\n‚ùå Testing CNBC gagal. Cek koneksi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTLjeh8T43aM",
        "outputId": "2cf021d5-6a4b-4c40-ac31-4cceb163a4c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ CNBC SCRAPER + TWEET HARVEST\n",
            "üéØ Target: Logam Mulia (Berita + Sosial Media)\n",
            "============================================================\n",
            "üß™ TESTING STRUKTUR CNBC\n",
            "==================================================\n",
            "‚úÖ Status: 200\n",
            "üìÑ Title: Hasil Pencarian  - CNBC Indonesia\n",
            "\n",
            "üîç Elemen artikel:\n",
            "  ‚Ä¢ div.gsc-webResult: 0\n",
            "  ‚Ä¢ div.search-result-item: 0\n",
            "  ‚Ä¢ article: 0\n",
            "\n",
            "üîç Pagination:\n",
            "\n",
            "üöÄ Testing CNBC OK! Mulai scraping berita...\n",
            "üöÄ MEMULAI SCRAPING CNBC INDONESIA - LOGAM MULIA\n",
            "============================================================\n",
            "üîç Menganalisis struktur halaman...\n",
            "  üíæ Debug HTML saved to 'debug_cnbc.html'\n",
            "  ‚ö†Ô∏è  Pagination tidak ditemukan, menggunakan default: 5 halaman\n",
            "\n",
            "üíæ Membuat file CSV: cnbc_logam_mulia_20250926_0719.csv\n",
            "\n",
            "üìñ Halaman 1/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 1\n",
            "\n",
            "üìñ Halaman 2/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=2...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 2\n",
            "\n",
            "üìñ Halaman 3/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=3...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 3\n",
            "\n",
            "üìñ Halaman 4/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=4...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 4\n",
            "\n",
            "üìñ Halaman 5/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=5...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 5\n",
            "\n",
            "üéâ SCRAPING SELESAI! Total: 0 artikel\n",
            "üíæ File: cnbc_logam_mulia_20250926_0719.csv\n",
            "üì• Untuk download: Buka folder proyek atau copy cnbc_logam_mulia_20250926_0719.csv\n",
            "\n",
            "‚úÖ Berita selesai: 0 artikel di cnbc_logam_mulia_20250926_0719.csv\n",
            "\n",
            "üê¶ Mulai tweet harvest? (Ya/Tidak)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import csv"
      ],
      "metadata": {
        "id": "tpiIVoH55fkr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5nur_wt313H",
        "outputId": "e9e10452-22de-4f39-e83a-e1c0ca1b79a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ CNBC SCRAPER FIXED\n",
            "üéØ Target: Logam Mulia\n",
            "============================================================\n",
            "üß™ TESTING STRUKTUR CNBC\n",
            "==================================================\n",
            "‚úÖ Status: 200\n",
            "üìÑ Title: Hasil Pencarian  - CNBC Indonesia\n",
            "\n",
            "üîç Elemen artikel:\n",
            "  ‚Ä¢ div.gsc-webResult: 0\n",
            "  ‚Ä¢ div.search-result-item: 0\n",
            "  ‚Ä¢ article: 0\n",
            "\n",
            "üîç Pagination:\n",
            "\n",
            "üöÄ Testing OK! Mulai scraping...\n",
            "üöÄ MEMULAI SCRAPING CNBC INDONESIA - LOGAM MULIA\n",
            "============================================================\n",
            "üîç Menganalisis struktur halaman...\n",
            "  üíæ Debug HTML saved to 'debug_cnbc.html'\n",
            "  ‚ö†Ô∏è  Pagination tidak ditemukan, menggunakan default: 5 halaman\n",
            "\n",
            "üíæ Membuat file CSV: cnbc_logam_mulia_20250926_0714.csv\n",
            "\n",
            "üìñ Halaman 1/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 1\n",
            "\n",
            "üìñ Halaman 2/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=2...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 2\n",
            "\n",
            "üìñ Halaman 3/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=3...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 3\n",
            "\n",
            "üìñ Halaman 4/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=4...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 4\n",
            "\n",
            "üìñ Halaman 5/5\n",
            "  üåê URL: https://www.cnbcindonesia.com/search?q=logam+mulia&page=5...\n",
            "    ‚ö†Ô∏è  Tidak ada artikel di halaman 5\n",
            "\n",
            "üéâ SCRAPING SELESAI! Total: 0 artikel\n",
            "üíæ File: cnbc_logam_mulia_20250926_0714.csv\n",
            "üì• Untuk download: Buka folder proyek atau copy cnbc_logam_mulia_20250926_0714.csv\n",
            "\n",
            "‚úÖ Selesai: 0 artikel di cnbc_logam_mulia_20250926_0714.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Fix headers (hapus spasi di User-Agent)\n",
        "headers = {\n",
        "    'User -Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Setup session dengan retry untuk handle error transient\n",
        "session = requests.Session()\n",
        "retry_strategy = Retry(\n",
        "    total=3,\n",
        "    backoff_factor=1,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        ")\n",
        "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "session.mount(\"http://\", adapter)\n",
        "session.mount(\"https://\", adapter)\n",
        "\n",
        "# Fungsi ambil isi artikel - DIPERBAIKI dengan selector CNBC terkini\n",
        "def get_article_content(link):\n",
        "    try:\n",
        "        print(f\"  üìñ Mengambil konten: {link[:50]}...\")\n",
        "        res = session.get(link, headers=headers, timeout=15)  # Naikkan timeout\n",
        "        res.raise_for_status()\n",
        "        soup = BeautifulSoup(res.text, 'lxml')  # Fallback: 'html.parser' jika lxml error\n",
        "\n",
        "        # Selector CNBC terkini untuk konten (div#detail-tab atau article p)\n",
        "        content_selectors = [\n",
        "            'div#detail-tab',\n",
        "            'div.content-detail',\n",
        "            'div.detail-text',\n",
        "            'article p',\n",
        "            'div[class*=\"content\"] p'\n",
        "        ]\n",
        "\n",
        "        article_content = \"\"\n",
        "        for selector in content_selectors:\n",
        "            div_content = soup.select_one(selector)\n",
        "            if div_content:\n",
        "                paragraphs = div_content.find_all('p')\n",
        "                if paragraphs:\n",
        "                    article_content = ' '.join([p.get_text(strip=True) for p in paragraphs[:10]])  # Limit paragraf\n",
        "                    break\n",
        "\n",
        "        # Fallback: Ambil semua p di body\n",
        "        if not article_content:\n",
        "            all_paragraphs = soup.find_all('p')\n",
        "            if len(all_paragraphs) > 3:\n",
        "                article_content = ' '.join([p.get_text(strip=True) for p in all_paragraphs[:5]])\n",
        "\n",
        "        return article_content[:800] if article_content else \"Konten tidak ditemukan\"  # Naikkan limit\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Error artikel: {link[:30]}... | {e}\")\n",
        "        return \"Error mengambil konten\"\n",
        "\n",
        "def scrape_cnbc_logam_mulia():\n",
        "    \"\"\"Fungsi utama untuk scraping CNBC - FIXED URL dan selector\"\"\"\n",
        "    print(\"üöÄ MEMULAI SCRAPING CNBC INDONESIA - LOGAM MULIA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # URL search CNBC terkini (pakai q= bukan query=; no fromdate/todate)\n",
        "    base_url = 'https://www.cnbcindonesia.com/search'\n",
        "    search_params = {'q': 'logam mulia'}  # Keyword utama\n",
        "\n",
        "    print(\"üîç Menganalisis struktur halaman...\")\n",
        "\n",
        "    try:\n",
        "        # Request halaman pertama dengan session\n",
        "        res_first = session.get(base_url, params=search_params, headers=headers, timeout=15)\n",
        "        res_first.raise_for_status()\n",
        "        soup_first = BeautifulSoup(res_first.text, 'lxml')\n",
        "\n",
        "        # Debug: Simpan HTML\n",
        "        with open('debug_cnbc.html', 'w', encoding='utf-8') as f:\n",
        "            f.write(soup_first.prettify())\n",
        "        print(\"  üíæ Debug HTML saved to 'debug_cnbc.html'\")\n",
        "\n",
        "        # Cari pagination - FIXED selector untuk CNBC (ul.pager atau div.pagination)\n",
        "        page_numbers = []\n",
        "        pagination_selectors = [\n",
        "            'ul.pager li a',\n",
        "            'div.pagination a',\n",
        "            'nav a[aria-label*=\"page\"]',\n",
        "            'a[href*=\"page=\"]'\n",
        "        ]\n",
        "\n",
        "        for selector in pagination_selectors:\n",
        "            links = soup_first.select(selector)\n",
        "            for a in links:\n",
        "                text = a.get_text(strip=True)\n",
        "                if text.isdigit() and int(text) > 1:\n",
        "                    page_numbers.append(int(text))\n",
        "\n",
        "                href = a.get('href', '')\n",
        "                page_match = re.search(r'page=(\\d+)', href, re.I)\n",
        "                if page_match:\n",
        "                    page_numbers.append(int(page_match.group(1)))\n",
        "\n",
        "        if page_numbers:\n",
        "            last_page = max(page_numbers)\n",
        "            print(f\"  üìÑ Total halaman ditemukan: {last_page}\")\n",
        "        else:\n",
        "            last_page = 5  # Default\n",
        "            print(f\"  ‚ö†Ô∏è  Pagination tidak ditemukan, menggunakan default: {last_page} halaman\")\n",
        "\n",
        "        last_page = min(last_page, 10)  # Batasi testing\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error analisis: {e}\")\n",
        "        last_page = 3\n",
        "\n",
        "    # Setup CSV dengan DictWriter untuk robustness\n",
        "    csv_filename = f'cnbc_logam_mulia_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
        "    print(f\"\\nüíæ Membuat file CSV: {csv_filename}\")\n",
        "\n",
        "    with open(csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        fieldnames = ['No', 'Title', 'Link', 'Time', 'Content', 'Scraped_Date']\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        total_articles = 0\n",
        "\n",
        "        for page in range(1, last_page + 1):\n",
        "            print(f\"\\nüìñ Halaman {page}/{last_page}\")\n",
        "\n",
        "            try:\n",
        "                # FIXED URL pagination: ?q=logam+mulia&page=N\n",
        "                params = search_params.copy()\n",
        "                if page > 1:\n",
        "                    params['page'] = page\n",
        "                url = base_url\n",
        "                print(f\"  üåê URL: {base_url}?q=logam+mulia&page={page if page > 1 else ''}...\")\n",
        "\n",
        "                res = session.get(url, params=params, headers=headers, timeout=15)\n",
        "                res.raise_for_status()\n",
        "                soup = BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "                # FIXED selector artikel: CNBC pakai div.gsc-webResult atau div.search-result\n",
        "                article_selectors = [\n",
        "                    'div.gsc-webResult',\n",
        "                    'div.search-result-item',\n",
        "                    'div[class*=\"result\"]',\n",
        "                    'article',\n",
        "                    'li.result-item'\n",
        "                ]\n",
        "\n",
        "                articles = []\n",
        "                for selector in article_selectors:\n",
        "                    found = soup.select(selector)\n",
        "                    if len(found) > 1:  # Minimal 1-2\n",
        "                        articles = found\n",
        "                        print(f\"    ‚úÖ Selector: {selector} ({len(articles)} items)\")\n",
        "                        break\n",
        "\n",
        "                if not articles:\n",
        "                    print(f\"    ‚ö†Ô∏è  Tidak ada artikel di halaman {page}\")\n",
        "                    continue\n",
        "\n",
        "                page_count = 0\n",
        "                for i, art in enumerate(articles[:15]):  # Batasi 15 per halaman\n",
        "                    try:\n",
        "                        # FIXED: Cari a_tag dan title\n",
        "                        a_tag = art.find('a', href=True)\n",
        "                        if not a_tag:\n",
        "                            continue\n",
        "\n",
        "                        title_elem = a_tag.find(['h1', 'h2', 'h3']) or a_tag\n",
        "                        title = title_elem.get_text(strip=True)[:150]  # Limit title\n",
        "\n",
        "                        link = a_tag['href']\n",
        "                        if link.startswith('/'):\n",
        "                            link = 'https://www.cnbcindonesia.com' + link\n",
        "\n",
        "                        # Time: Cari span time atau fallback\n",
        "                        time_elem = art.find('span', class_=re.compile(r'time|date|published'))\n",
        "                        time_info = time_elem.get_text(strip=True) if time_elem else datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "                        # Filter keyword\n",
        "                        if not any(kw in title.lower() for kw in ['logam', 'emas', 'perak', 'mulia']):\n",
        "                            continue\n",
        "\n",
        "                        content = get_article_content(link) if link else \"Link invalid\"\n",
        "\n",
        "                        # Write row\n",
        "                        row = {\n",
        "                            'No': total_articles + 1,\n",
        "                            'Title': title,\n",
        "                            'Link': link,\n",
        "                            'Time': time_info,\n",
        "                            'Content': content,\n",
        "                            'Scraped_Date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        }\n",
        "                        writer.writerow(row)\n",
        "\n",
        "                        total_articles += 1\n",
        "                        page_count += 1\n",
        "                        print(f\"    ‚úÖ [{total_articles}] {title[:50]}...\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ùå Error artikel {i}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                print(f\"  üìä Halaman {page}: {page_count} artikel\")\n",
        "                time.sleep(3)  # Delay lebih panjang\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Gagal halaman {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\nüéâ SCRAPING SELESAI! Total: {total_articles} artikel\")\n",
        "    print(f\"üíæ File: {csv_filename}\")\n",
        "\n",
        "    # Manual download info (no Colab dependency)\n",
        "    print(f\"üì• Untuk download: Buka folder proyek atau copy {csv_filename}\")\n",
        "    return csv_filename, total_articles\n",
        "\n",
        "# Fungsi testing - FIXED\n",
        "def test_cnbc_structure():\n",
        "    \"\"\"Test struktur - FIXED URL\"\"\"\n",
        "    print(\"üß™ TESTING STRUKTUR CNBC\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    url = 'https://www.cnbcindonesia.com/search?q=logam+mulia'\n",
        "\n",
        "    try:\n",
        "        res = session.get(url, headers=headers, timeout=10)\n",
        "        res.raise_for_status()\n",
        "        soup = BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "        print(f\"‚úÖ Status: {res.status_code}\")\n",
        "        print(f\"üìÑ Title: {soup.title.text if soup.title else 'No title'}\")\n",
        "\n",
        "        # Test selectors\n",
        "        print(\"\\nüîç Elemen artikel:\")\n",
        "        selectors = ['div.gsc-webResult', 'div.search-result-item', 'article']\n",
        "        for sel in selectors:\n",
        "            count = len(soup.select(sel))\n",
        "            print(f\"  ‚Ä¢ {sel}: {count}\")\n",
        "\n",
        "        print(\"\\nüîç Pagination:\")\n",
        "        pag_selectors = ['ul.pager', 'div.pagination']\n",
        "        for sel in pag_selectors:\n",
        "            count = len(soup.select(sel))\n",
        "            if count > 0:\n",
        "                print(f\"  ‚Ä¢ {sel}: {count} found\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# EKSEKUSI UTAMA\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ CNBC SCRAPER FIXED\")\n",
        "    print(\"üéØ Target: Logam Mulia\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Run testing dulu\n",
        "    if test_cnbc_structure():\n",
        "        print(\"\\nüöÄ Testing OK! Mulai scraping...\")\n",
        "        filename, count = scrape_cnbc_logam_mulia()\n",
        "        print(f\"\\n‚úÖ Selesai: {count} artikel di {filename}\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Testing gagal. Cek koneksi/internet atau update selector.\")\n"
      ]
    }
  ]
}